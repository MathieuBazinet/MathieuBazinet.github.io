---
layout: archive
title: "Mes publications"
lang: fr
permalink: /fr/publications/
author_profile: true
ref: publication
page_css:
  - /assets/css/publications.css
---

{% if site.author.googlescholar %}
  <div class="wordwrap">Vous pouvez trouver tous mes articles sur <a href="{{site.author.googlescholar}}">mon profil Google Scholar</a>.</div>
{% endif %}

{% if page.page_css %}
  {% for stylesheet in page.page_css %}
    <link rel="stylesheet" href="{{ stylesheet | relative_url }}">
  {% endfor %}
{% endif %}

{% include base_path %}


<section class="publications">
  <h2>Articles</h2>

  <div class="pub-grid">
    <article class="pub-card" data-url="https://proceedings.mlr.press/v267/leblanc25a.html">
      <h3>Generalization Bounds via Meta-Learned Model Representations: PAC-Bayes and Sample Compression Hypernetworks</h3>
      <p class="authors">Benjamin Leblanc, <strong>Mathieu Bazinet</strong>, Nathaniel D'Amours, Alexandre Drouin, Pascal Germain</p>
      <p class="venue">ICML 2025</p>
      <p class="note">Version longue du papier <i>Sample Compression Hypernetworks: From Generalization Bounds to Meta-Learning</i>.
      <div class="links">
        <a href="https://openreview.net/forum?id=Qn6yZb5iLC">OpenReview</a>
        <a href="https://arxiv.org/abs/2410.13577">ArXiv</a>
        <a href="https://github.com/GRAAL-Research/DeepRM">Code</a>
      </div>
    </article>

    <article class="pub-card" data-url="https://proceedings.mlr.press/v258/bazinet25a.html">
      <h3>Sample Compression Unleashed: New Generalization Bounds for Real Valued Losses</h3>
      <p class="authors"><strong>Mathieu Bazinet</strong>, Valentina Zantedeschi, Pascal Germain</p>
      <p class="venue">AISTATS 2025</p>
      <div class="links">
        <a href="https://openreview.net/forum?id=0ynSy2dwNi">OpenReview</a>
        <a href="https://arxiv.org/abs/2409.17932">ArXiv</a>
        <a href="https://github.com/GRAAL-Research/pick-to-learn">Code</a>
      </div>
    </article>

    <article class="pub-card" data-url="https://doi.org/10.1016/j.seppur.2024.130877">
      <h3>Application of machine learning tools to study the synergistic impact of physicochemical properties.</h3>
      <p class="authors">Zain Sanchez-Reinoso, <strong>Mathieu Bazinet</strong>, Benjamin Leblanc, Jean-Pierre Clément, Pascal Germain, Laurent Bazinet</p>
      <p class="venue">Separation and Purification Technology, Dec 2024</p>
      <div class="links">
        <a href="https://www.sciencedirect.com/science/article/pii/S1383586624046161">Lien vers le papier</a>
        <a href="https://github.com/MathieuBazinet/EDFM">Code</a>
      </div>
    </article>

    <article class="pub-card" data-url="https://doi.org/10.1016/j.foodres.2024.115417">
      <h3>How peptide migration and fraction bioactivity are modulated by electrical current modes...</h3>
      <p class="authors">Aurore Cournoyer, <strong>Mathieu Bazinet</strong>, Jean-Pierre Clément, Pier-Luc Plante, Ismail Fliss, Laurent Bazinet</p>
      <p class="venue">Food Research International, Nov 2024</p>
      <div class="links">
        <a href="https://www.sciencedirect.com/science/article/pii/S096399692401487X">Lien vers le papier</a>
        <a href="https://github.com/MathieuBazinet/pef-ml">Code</a>
      </div>
    </article>

  </div>
</section>

<section class="publications">
  <h2>Preprints</h2>

  <div class="pub-grid">
    <article class="pub-card" data-url="https://arxiv.org/abs/2503.10503">
      <h3>Sample Compression for Self-Certified Continual Learning</h3>
      <p class="authors">Jacob Comeau*, <strong>Mathieu Bazinet</strong>*, Pascal Germain, Cem Subakan</p>
      <!-- <p class="venue">ICML 2025</p> -->
      <p class="note">* indique une contribution égale.</p>
      <div class="links">
        <a href="https://arxiv.org/abs/2503.10503">Link to paper</a>
      </div>
    </article>

      <article class="pub-card">
      <h3>Wavelet-Based Feature Map for Kernel Approximation.</h3>
      <p class="authors"><strong>Mathieu Bazinet</strong>, Valentina Zantedeschi, Pascal Germain</p>
      <!-- <p class="venue">ICML 2025</p> -->
      <p class="note">Cet article a été soumis en 2023, mais n'a jamais été publié.</p>
    </article>
  </div>
</section>

<section class="publications">
  <h2>Workshop papers</h2>

  <div class="pub-grid">
    <article class="pub-card" data-url="https://openreview.net/forum?id=QOl18v8Gfq">
      <h3>Sample Compression Unleashed : New Generalization Bounds for Real Valued Losses.</h3>
      <p class="authors"><strong>Mathieu Bazinet</strong>, Valentina Zantedeschi, Pascal Germain</p>
      <p class="venue">Workshop on Mathematics of Modern Machine Learning of NeurIPS 2024</p>
      <div class="links">
          <a href="https://openreview.net/forum?id=QOl18v8Gfq">OpenReview</a>
          <a href="/files/poster_sample_compress_neurips.pdf">Poster</a>
      </div>
    </article>

    <article class="pub-card" data-url="https://openreview.net/forum?id=rutp5J4ks2">
      <h3>Sample Compression Unleashed : New Generalization Bounds for Real Valued Losses.</h3>
      <p class="authors"><strong>Mathieu Bazinet</strong>, Valentina Zantedeschi, Pascal Germain</p>
      <p class="venue">Workshop on Machine Learning and Compression of NeurIPS 2024</p>
      <div class="links">
          <a href="https://openreview.net/forum?id=rutp5J4ks2">OpenReview forum</a>
          <a href="/files/poster_sample_compress_neurips.pdf">Poster</a>
      </div>
    </article>

    <article class="pub-card" data-url="https://openreview.net/forum?id=7v8HzCUVbh">
      <h3>Sample Compression Hypernetworks: From Generalization Bounds to Meta-Learning.</h3>
      <p class="authors">Benjamin Leblanc, <strong>Mathieu Bazinet</strong>, Nathaniel D'Amours, Alexandre Drouin, Pascal Germain</p>
      <p class="venue">Workshop on Machine Learning and Compression of NeurIPS 2024</p>
      <div class="links">
          <a href="https://openreview.net/forum?id=7v8HzCUVbh">OpenReview</a>
          <a href="/files/poster_sample_compression_hypernetwork_neurips.pdf">Poster</a>
      </div>
    </article>
  </div>
</section>

